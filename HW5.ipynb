{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pplt\n",
    "from scipy.cluster.vq import vq\n",
    "import os\n",
    "from os import path\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as pplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_path = \"C:\\\\Users\\\\tanghao\\\\Desktop\\\\ADL\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list for all class\n",
    "def activity_list(path):\n",
    "    ADL_list = os.listdir(path)\n",
    "    return ADL_list\n",
    "\n",
    "## This one has overlap function, LET\"S CHANGE THE ABOVE METHOD!!!!!!!!!!!!!!!!!!!!!S\n",
    "def chop_and_append_v2(path, segment, list_of_files, overlap): ## This return a array. Make sure to find a way to \n",
    "    X = overlap; class_all_segments = np.array([[0] * 96])\n",
    "    for i in list_of_files:\n",
    "        temp_file = np.array(pd.read_csv(os.path.join(path, i), delimiter = \" \", header = None))\n",
    "        T = len(temp_file); U = segment; N = int((T-X*U)/(1-X)/U)\n",
    "        if (int((1-X)*U) * N + U < T): ## Here I calculate the remaining tuples and utilize them to make more vectors\n",
    "            N = N+1+int((T - int((1-X)*U) * N - U)/int(U*(1-X)))\n",
    "        temp_array = np.array([[0]*96]); origin = 0\n",
    "        for i in range(N):\n",
    "            temp_array = np.concatenate((temp_array,temp_file[origin:origin+segment,:].reshape(1,segment * 3)))\n",
    "            origin = int(origin + segment * (1-X))\n",
    "        class_all_segments = np.concatenate([class_all_segments, temp_array[1:,:]]) # [] and (), both work\n",
    "    return class_all_segments[1:,:]\n",
    "## Segmentation of Vectors\n",
    "def segmentation_of_vector(class_path, seg, class_list, overlap):  #cut every txt to approprite length for segment X integer, put them into one array\n",
    "    for i in class_list:\n",
    "        vars()[i + '_vector'] = chop_and_append_v2(os.path.join(class_path, i), segment, os.listdir(os.path.join(class_path, i)), overlap)    \n",
    "    feature = np.concatenate(\n",
    "        [vars()[class_list[0] + '_vector'], vars()[class_list[1] + '_vector'], vars()[class_list[2] + '_vector'], vars()[class_list[3] + '_vector']\n",
    "            , vars()[class_list[4] + '_vector'], vars()[class_list[5] + '_vector'], vars()[class_list[6] + '_vector'], vars()[class_list[7] + '_vector'],\n",
    "         vars()[class_list[8] + '_vector'],\n",
    "         vars()[class_list[9] + '_vector'], vars()[class_list[10] + '_vector'], vars()[class_list[11] + '_vector'],\n",
    "         vars()[class_list[12] + '_vector'], vars()[class_list[13] + '_vector']])\n",
    "    return feature\n",
    "## K_fold\n",
    "def k_fold(classpath, class_list,seed):\n",
    "    all_k_fold = [[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    random.seed(seed)\n",
    "    for c in range(len(class_list)):\n",
    "        l = list(os.listdir(classpath + class_list[c]))\n",
    "        l1 = random.sample(l,int(len(l)/3))\n",
    "        l2 = random.sample(list(set(l)-set(l1)),int(len(l)/3))\n",
    "        l3 = list(set(l)-set(l1)-set(l2)) \n",
    "        all_k_fold[c] = [l1,l2,l3]\n",
    "    return all_k_fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "class_list = activity_list(class_path)\n",
    "kfold_list = k_fold(class_path, class_list, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram\n",
    "def generating_histogram(dictionary, class_path, category, cluster_number, class_list, segment, train_file, test_file, overlap):\n",
    "    label = np.nan\n",
    "    for i in range(len(class_list)):\n",
    "        if (class_list[i] == category):\n",
    "            label = i      \n",
    "    hist_train = np.array([[0] * (cluster_number + 1)])\n",
    "    hist_test = np.array([[0] * (cluster_number + 1)])\n",
    "    for f in train_file:\n",
    "        segments_in_file = chop_and_append_v2(os.path.join(class_path + category), segment, [f], overlap)\n",
    "        segment_with_cluster_index = vq(segments_in_file, dictionary.cluster_centers_)[0]\n",
    "        histogram = [0] * (cluster_number + 1)  ## This list is going to be the histogram's container later.\n",
    "        for s in range(len(segment_with_cluster_index)):\n",
    "            histogram[segment_with_cluster_index[s]] += 1\n",
    "        histogram[cluster_number] = label\n",
    "        hist_train = np.concatenate((hist_train, np.array([histogram])))\n",
    "    hist_train_array = hist_train[1:,:].copy()\n",
    "    for f in test_file:\n",
    "        segments_in_file = chop_and_append_v2(os.path.join(class_path + category), segment, [f], overlap)\n",
    "        segment_with_cluster_index = vq(segments_in_file, dictionary.cluster_centers_)[0]\n",
    "        histogram = [0] * (cluster_number + 1)  ## This list is going to be the histogram's container later.\n",
    "        for s in range(len(segment_with_cluster_index)):\n",
    "            histogram[segment_with_cluster_index[s]] += 1\n",
    "        histogram[cluster_number] = label\n",
    "        hist_test = np.concatenate((hist_test, np.array([histogram])))\n",
    "    hist_test_array = hist_test[1:,:].copy()    \n",
    "    return hist_train_array,hist_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_list is the k_fold of each class following the order of class_list\n",
    "# testfold should be 0, 1 or 2\n",
    "# np.concatenate(), the input array must be [[]] (np.array([[]])), then it can be merge, if one of them is only [], it doesn't work\n",
    "def classification(dictionary, class_path, cluster_number, class_list, segment, kfold_list, overlap):\n",
    "    fold = len(kfold_list[0]); Acc = 0; prediction = [[],[],[]]; Acc = []; test_label = []\n",
    "    rfc = RandomForestClassifier(max_depth = 240, n_estimators = 100, random_state = 9)\n",
    "    for f in range(fold):\n",
    "        all_hist_train = np.array([[0] * (cluster_number + 1)])\n",
    "        all_hist_test = np.array([[0] * (cluster_number + 1)])\n",
    "        trainfold = list(set([0,1,2]) - set([f]));  count = 0;\n",
    "        for c in range(len(class_list)):\n",
    "            category = class_list[c]; test_file = kfold_list[c][f];\n",
    "            train_file = list(np.concatenate((np.array(kfold_list[c][trainfold[0]]), np.array(kfold_list[c][trainfold[1]])))) \n",
    "            cat_train_hists, cat_test_hists = generating_histogram(dictionary, class_path, category, cluster_number, class_list, segment, train_file, test_file, overlap)\n",
    "            all_hist_train = np.concatenate((all_hist_train, cat_train_hists))\n",
    "            all_hist_test = np.concatenate((all_hist_test, cat_test_hists))\n",
    "        rfc.fit(all_hist_train[1:,:cluster_number], all_hist_train[1:,cluster_number])\n",
    "        predict_test = rfc.predict(all_hist_test[1:,:cluster_number])\n",
    "        test_label.append(all_hist_test[1:,cluster_number])\n",
    "        all_hist_test_a = all_hist_test[1:,:]\n",
    "        for i in range(len(all_hist_test)-1):\n",
    "            if (predict_test[i] == all_hist_test_a[i,cluster_number]):\n",
    "                count+=1\n",
    "        Acc.append(count/len(predict_test)); prediction[f].append(list(predict_test))\n",
    "    accuracy = np.mean(np.array(Acc)); Max_index = Acc.index(max(Acc))\n",
    "    print(pd.DataFrame(confusion_matrix(test_label[Max_index], prediction[Max_index][0])))\n",
    "    return prediction[Max_index][0], test_label[Max_index], Max_index, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "## calculate the experiment table\n",
    "SEGMENT = [16,32,48]\n",
    "OVERLAP = [0.0,0.5,0.9]\n",
    "CLUSTER_NUM = [120,240,360,480,960]\n",
    "\n",
    "PREDICT = [[],[],[]]\n",
    "MAX_ACCU = [[],[],[]]\n",
    "TRUTH = [[],[],[]]\n",
    "AVE_ACCU = [[],[],[]]\n",
    "Acc = [[],[],[]]\n",
    "\n",
    "for i in range(len(SEGMENT)):\n",
    "    for o in OVERLAP:\n",
    "        for c in CLUSTER_NUM:\n",
    "            f = segmentation_of_vector(class_path, SEGMENT[i], class_list, o)\n",
    "            dictionary = KMeans(n_clusters = c, random_state = 0).fit(f)\n",
    "            print(\"Segment: \" + str(SEGMENT[i]) + \" Overlap: \" + str(o) + \" Cluster_number: \" + str(c))\n",
    "            prediction, test_label, max_accu_fold, ave_accuracy = classification(dictionary, class_path, c, class_list, SEGMENT[i], kfold_list, o)\n",
    "            PREDICT[i].append(prediction)\n",
    "            TRUTH[i].append(test_label)\n",
    "            MAX_ACCU[i].append(max_accu_fold)\n",
    "            AVE_ACCU[i].append(ave_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-566870fabbb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmentation_of_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m#range(len(class_list))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfold_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfold_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 random_state=random_state)\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    442\u001b[0m     centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n\u001b[0;32m    443\u001b[0m                                             \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[0;32m    445\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\cluster\\_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;31m# Pairwise distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,\n\u001b[0m\u001b[0;32m    165\u001b[0m                         X_norm_squared=None):\n\u001b[0;32m    166\u001b[0m     \"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Generate the average histogram\n",
    "## enter the best cluster_number, segment, overlap !!!!!!!!!!!!!!!!\n",
    "## Do average first, then do density, in fact both way make sense. \n",
    "ave_historgram = []\n",
    "cluster_number = 120\n",
    "overlap = 0.9\n",
    "s = 48\n",
    "f = segmentation_of_vector(class_path, s, class_list, overlap)\n",
    "dictionary = KMeans(n_clusters = cluster_number, random_state = 0).fit(f)\n",
    "for i in range(len(class_list)):  #range(len(class_list))\n",
    "    train_file = list(np.concatenate((np.array(kfold_list[i][0]), np.array(kfold_list[i][1]))))\n",
    "    test_file = kfold_list[i][2]\n",
    "    hist1, hist2 = generating_histogram(dictionary, class_path, class_list[i], cluster_number, class_list, s, train_file, test_file, overlap)\n",
    "    hist = np.concatenate((hist1,hist2)) ## here is the histogram for the whole activity\n",
    "    mean_hist_class = np.mean(hist[:,:cluster_number], axis = 0)\n",
    "    ave_historgram.append(mean_hist_class)\n",
    "\n",
    "## Generate Histogram\n",
    "ax = [0] * 14\n",
    "fig = pplt.figure(figsize=(24,40))\n",
    "fig.subplots_adjust(hspace = 0.5)\n",
    "for i in range(len(ave_historgram)):    \n",
    "    ax[i] = fig.add_subplot(7, 2, i+1)    \n",
    "    ax[i].bar(np.arange(cluster_number), ave_historgram[i], align='center', alpha=0.5)\n",
    "    pplt.xlabel('Cluster Center', size = 18)\n",
    "    pplt.xticks(size = 16)    \n",
    "    pplt.ylabel('Count', size = 18)\n",
    "    pplt.yticks(size = 16)\n",
    "    pplt.title(class_list[i] + ' Histogram', size = 20)    \n",
    "fig.savefig('/folder/file')\n",
    "pplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the confusion matrix\n",
    "# segment = 48; overlap = 0.9; K = 120\n",
    "df = pd.DataFrame(confusion_matrix(TRUTH[1][14], PREDICT[1][14]))\n",
    "df.columns = class_list\n",
    "df.index = class_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate the maximum vectors quantity from each class/file/list of file\n",
    "def chop_and_append_v2(path, segment, list_of_files, overlap): ## This return a array. Make sure to find a way to \n",
    "    X = overlap; class_all_segments = np.array([[0] * 96])\n",
    "    for i in list_of_files:\n",
    "        temp_file = np.array(pd.read_csv(os.path.join(path, i), delimiter = \" \", header = None))\n",
    "        T = len(temp_file); U = segment; N = int((T-X*U)/(1-X)/U)\n",
    "        if (int((1-X)*U) * N + U < T): ## Here I calculate the remaining tuples and utilize them to make more vectors\n",
    "            N = N+1+int((T - int((1-X)*U) * N - U)/int(U*(1-X)))\n",
    "        temp_array = np.array([[0]*96]); origin = 0\n",
    "        for i in range(N):\n",
    "            temp_array = np.concatenate((temp_array,temp_file[origin:origin+segment,:].reshape(1,segment * 3)))\n",
    "            origin = int(origin + segment * (1-X))\n",
    "        class_all_segments = np.concatenate([class_all_segments, temp_array[1:,:]]) # [] and (), both work\n",
    "    return class_all_segments[1:,:]\n",
    "## Segmentation of Vectors, generate vectors with above function and make them into one list for VQ\n",
    "def segmentation_of_vector(class_path, seg, class_list, overlap):  #cut every txt to approprite length for segment X integer, put them into one array\n",
    "    for i in class_list:\n",
    "        vars()[i + '_vector'] = chop_and_append_v2(os.path.join(class_path, i), segment, os.listdir(os.path.join(class_path, i)), overlap)    \n",
    "    feature = np.concatenate(\n",
    "        [vars()[class_list[0] + '_vector'], vars()[class_list[1] + '_vector'], vars()[class_list[2] + '_vector'], vars()[class_list[3] + '_vector']\n",
    "            , vars()[class_list[4] + '_vector'], vars()[class_list[5] + '_vector'], vars()[class_list[6] + '_vector'], vars()[class_list[7] + '_vector'],\n",
    "         vars()[class_list[8] + '_vector'],\n",
    "         vars()[class_list[9] + '_vector'], vars()[class_list[10] + '_vector'], vars()[class_list[11] + '_vector'],\n",
    "         vars()[class_list[12] + '_vector'], vars()[class_list[13] + '_vector']])\n",
    "    return feature\n",
    "## K-Means\n",
    "dictionary = KMeans(n_clusters = c, random_state = 0).fit(f) ## f is the list with all vectors from above function\n",
    "\n",
    "## Histogram, this function's input require separated trainset and testset, overlap, segment\n",
    "## the dictionary,file-path and category(class), class-list\n",
    "def generating_histogram(dictionary, class_path, category, cluster_number, class_list, segment, train_file, test_file, overlap):\n",
    "    label = np.nan\n",
    "    for i in range(len(class_list)):\n",
    "        if (class_list[i] == category):\n",
    "            label = i      \n",
    "    hist_train = np.array([[0] * (cluster_number + 1)])\n",
    "    hist_test = np.array([[0] * (cluster_number + 1)])\n",
    "    for f in train_file:\n",
    "        segments_in_file = chop_and_append_v2(os.path.join(class_path + category), segment, [f], overlap)\n",
    "        segment_with_cluster_index = vq(segments_in_file, dictionary.cluster_centers_)[0]\n",
    "        histogram = [0] * (cluster_number + 1)  ## This list is going to be the histogram's container later.\n",
    "        for s in range(len(segment_with_cluster_index)):\n",
    "            histogram[segment_with_cluster_index[s]] += 1\n",
    "        histogram[cluster_number] = label\n",
    "        hist_train = np.concatenate((hist_train, np.array([histogram])))\n",
    "    hist_train_array = hist_train[1:,:].copy()\n",
    "    for f in test_file:\n",
    "        segments_in_file = chop_and_append_v2(os.path.join(class_path + category), segment, [f], overlap)\n",
    "        segment_with_cluster_index = vq(segments_in_file, dictionary.cluster_centers_)[0]\n",
    "        histogram = [0] * (cluster_number + 1)  ## This list is going to be the histogram's container later.\n",
    "        for s in range(len(segment_with_cluster_index)):\n",
    "            histogram[segment_with_cluster_index[s]] += 1\n",
    "        histogram[cluster_number] = label\n",
    "        hist_test = np.concatenate((hist_test, np.array([histogram])))\n",
    "    hist_test_array = hist_test[1:,:].copy()    \n",
    "    return hist_train_array,hist_test_array\n",
    "## use length = 48, K = 120, Overlap = 0.9\n",
    "ave_historgram = []; cluster_number = 120 ;overlap = 0.9 ;s = 48\n",
    "f = segmentation_of_vector(class_path, s, class_list, overlap)\n",
    "dictionary = KMeans(n_clusters = cluster_number, random_state = 0).fit(f)\n",
    "for i in range(len(class_list)):  #range(len(class_list))\n",
    "    print(i)\n",
    "    train_file = list(np.concatenate((np.array(kfold_list[i][0]), np.array(kfold_list[i][1]))))\n",
    "    test_file = kfold_list[i][2]\n",
    "    hist1, hist2 = generating_histogram(dictionary, class_path, class_list[i], cluster_number, class_list, s, train_file, test_file, overlap)\n",
    "    hist = np.concatenate((hist1,hist2)) ## here is the histogram for the whole activity\n",
    "    mean_hist_class = np.mean(hist[:,:cluster_number], axis = 0) ## Thiis is the mean histogram of each activity\n",
    "    ave_historgram.append(mean_hist_class)\n",
    "## Generate Histogram\n",
    "ax = [0] * 14\n",
    "fig = pplt.figure(figsize=(24,40))\n",
    "fig.subplots_adjust(hspace = 0.5)\n",
    "for i in range(len(ave_historgram)):    \n",
    "    ax[i] = fig.add_subplot(7, 2, i+1)    \n",
    "    ax[i].bar(np.arange(cluster_number), ave_historgram[i], align='center', alpha=0.5)\n",
    "    pplt.xlabel('Cluster Center', size = 18)\n",
    "    pplt.xticks(size = 16)    \n",
    "    pplt.ylabel('Count', size = 18)\n",
    "    pplt.yticks(size = 16)\n",
    "    pplt.title(class_list[i] + ' Histogram', size = 20)    \n",
    "fig.savefig('/folder/file')\n",
    "\n",
    "# Classification\n",
    "def classification(dictionary, class_path, cluster_number, class_list, segment, kfold_list, overlap):\n",
    "    fold = len(kfold_list[0]); Acc = 0; prediction = [[],[],[]]; Acc = []; test_label = []\n",
    "    rfc = RandomForestClassifier(max_depth = 240, n_estimators = 100, random_state = 9)\n",
    "    for f in range(fold):\n",
    "        all_hist_train = np.array([[0] * (cluster_number + 1)])\n",
    "        all_hist_test = np.array([[0] * (cluster_number + 1)])\n",
    "        trainfold = list(set([0,1,2]) - set([f]));  count = 0;\n",
    "        for c in range(len(class_list)):\n",
    "            category = class_list[c]; test_file = kfold_list[c][f];\n",
    "            train_file = list(np.concatenate((np.array(kfold_list[c][trainfold[0]]), np.array(kfold_list[c][trainfold[1]])))) \n",
    "            cat_train_hists, cat_test_hists = generating_histogram(dictionary, class_path, category, cluster_number, class_list, segment, train_file, test_file, overlap)\n",
    "            all_hist_train = np.concatenate((all_hist_train, cat_train_hists))\n",
    "            all_hist_test = np.concatenate((all_hist_test, cat_test_hists))\n",
    "        rfc.fit(all_hist_train[1:,:cluster_number], all_hist_train[1:,cluster_number])\n",
    "        predict_test = rfc.predict(all_hist_test[1:,:cluster_number])\n",
    "        test_label.append(all_hist_test[1:,cluster_number])\n",
    "        all_hist_test_a = all_hist_test[1:,:]\n",
    "        for i in range(len(all_hist_test)-1):\n",
    "            if (predict_test[i] == all_hist_test_a[i,cluster_number]):\n",
    "                count+=1\n",
    "        Acc.append(count/len(predict_test)); prediction[f].append(list(predict_test))\n",
    "    accuracy = np.mean(np.array(Acc)); Max_index = Acc.index(max(Acc))\n",
    "    print(pd.DataFrame(confusion_matrix(test_label[Max_index], prediction[Max_index][0])))\n",
    "    return prediction[Max_index][0], test_label[Max_index], Max_index, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
